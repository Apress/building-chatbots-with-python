{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.11'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 7.9MB/s ta 0:00:0111\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /usr/local/lib/python2.7/site-packages\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Creating a shortcut link for 'en' didn't work (maybe you don't have\n",
      "    admin permissions?), but you can still load the model via its full\n",
      "    package name: nlp = spacy.load('{name}')\u001b[0m\n",
      "    Download successful but linking failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en #For Python2 installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am VERB\n",
      "learning VERB\n",
      "how ADV\n",
      "to PART\n",
      "build VERB\n",
      "chatbots NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en') #Loads the spacy en model into a python object \n",
    "doc = nlp(u'I am learning how to build chatbots') #Creates a Doc object \n",
    "for token in doc:  \n",
    "    print(token.text, token.pos_) #prints the text and POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am VERB\n",
      "going VERB\n",
      "to ADP\n",
      "London PROPN\n",
      "next ADJ\n",
      "week NOUN\n",
      "for ADP\n",
      "a DET\n",
      "meeting NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am going to London next week for a meeting.')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google google PROPN NNP compound Xxxxx True False\n",
      "release release NOUN NN nmod xxxx True False\n",
      "\" \" PUNCT `` punct \" False False\n",
      "Move move PROPN NNP nmod Xxxx True False\n",
      "Mirror mirror PROPN NNP nmod Xxxxx True False\n",
      "\" \" PUNCT '' punct \" False False\n",
      "AI ai PROPN NNP compound XX True False\n",
      "experiment experiment NOUN NN ROOT xxxx True False\n",
      "that that ADJ WDT nsubj xxxx True True\n",
      "matches match VERB VBZ relcl xxxx True False\n",
      "your -PRON- ADJ PRP$ poss xxxx True True\n",
      "pose pose NOUN NN dobj xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "80,000 80,000 NUM CD nummod dd,ddd False False\n",
      "images image NOUN NNS pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Google release \"Move Mirror\" AI experiment that matches your pose from 80,000 images')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "data = [(u'Google', u'google', u'PROPN', u'NNP', u'compound', u'Xxxxx', True, False),\n",
    "(u'release', u'release', u'NOUN', u'NN', u'nmod', u'xxxx', True, False),\n",
    "(u'\"', u'\"', u'PUNCT', u'``', u'punct', u'\"', False, False),\n",
    "(u'Move', u'move', u'PROPN', u'NNP', u'nmod', u'Xxxx', True, False),\n",
    "(u'Mirror', u'mirror', u'PROPN', u'NNP', u'nmod', u'Xxxxx', True, False),\n",
    "(u'\"', u'\"', u'PUNCT', u\"''\", u'punct', u'\"', False, False),\n",
    "(u'AI', u'ai', u'PROPN', u'NNP', u'compound', u'XX', True, False),\n",
    "(u'experiment', u'experiment', u'NOUN', u'NN', u'ROOT', u'xxxx', True, False),\n",
    "(u'that', u'that', u'ADJ', u'WDT', u'nsubj', u'xxxx', True, True),\n",
    "(u'matches', u'match', u'VERB', u'VBZ', u'relcl', u'xxxx', True, False),\n",
    "(u'your', u'-PRON-', u'ADJ', u'PRP$', u'poss', u'xxxx', True, True),\n",
    "(u'pose', u'pose', u'NOUN', u'NN', u'dobj', u'xxxx', True, False),\n",
    "(u'from', u'from', u'ADP', u'IN', u'prep', u'xxxx', True, True),\n",
    "(u'80,000', u'80,000', u'NUM', u'CD', u'nummod', u'dd,ddd', False, False),\n",
    "(u'images', u'image', u'NOUN', u'NNS', u'pobj', u'xxxx', True, False)]\n",
    "\n",
    "def display_as_table(headers, data):\n",
    "    html = '<table border=\"2\"><tr>' + \"<th>{}</th>\"*len(headers) + '</tr>'\n",
    "    html = html.format(*headers)\n",
    "\n",
    "    html += \"\"\"<tr>{}</tr></table>\"\"\".format(\n",
    "        '</tr><tr>'.join(\n",
    "            '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n",
    "        )\n",
    "    \n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"2\"><tr><th>TEXT</th><th>LEMMA</th><th>POS</th><th>TAG</th><th>DEP</th><th>SHAPE</th><th>ALPHA</th><th>SHAPE</th></tr><tr><td>Google</td><td>google</td><td>PROPN</td><td>NNP</td><td>compound</td><td>Xxxxx</td><td>True</td><td>False</td></tr><tr><td>release</td><td>release</td><td>NOUN</td><td>NN</td><td>nmod</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>\"</td><td>\"</td><td>PUNCT</td><td>``</td><td>punct</td><td>\"</td><td>False</td><td>False</td></tr><tr><td>Move</td><td>move</td><td>PROPN</td><td>NNP</td><td>nmod</td><td>Xxxx</td><td>True</td><td>False</td></tr><tr><td>Mirror</td><td>mirror</td><td>PROPN</td><td>NNP</td><td>nmod</td><td>Xxxxx</td><td>True</td><td>False</td></tr><tr><td>\"</td><td>\"</td><td>PUNCT</td><td>''</td><td>punct</td><td>\"</td><td>False</td><td>False</td></tr><tr><td>AI</td><td>ai</td><td>PROPN</td><td>NNP</td><td>compound</td><td>XX</td><td>True</td><td>False</td></tr><tr><td>experiment</td><td>experiment</td><td>NOUN</td><td>NN</td><td>ROOT</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>that</td><td>that</td><td>ADJ</td><td>WDT</td><td>nsubj</td><td>xxxx</td><td>True</td><td>True</td></tr><tr><td>matches</td><td>match</td><td>VERB</td><td>VBZ</td><td>relcl</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>your</td><td>-PRON-</td><td>ADJ</td><td>PRP$</td><td>poss</td><td>xxxx</td><td>True</td><td>True</td></tr><tr><td>pose</td><td>pose</td><td>NOUN</td><td>NN</td><td>dobj</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>from</td><td>from</td><td>ADP</td><td>IN</td><td>prep</td><td>xxxx</td><td>True</td><td>True</td></tr><tr><td>80,000</td><td>80,000</td><td>NUM</td><td>CD</td><td>nummod</td><td>dd,ddd</td><td>False</td><td>False</td></tr><tr><td>images</td><td>image</td><td>NOUN</td><td>NNS</td><td>pobj</td><td>xxxx</td><td>True</td><td>False</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_as_table(['TEXT', 'LEMMA', 'POS', 'TAG', 'DEP', 'SHAPE', 'ALPHA', 'SHAPE'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chuckle']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer \n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES \n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES) \n",
    "lemmatizer('chuckles', 'NOUN') # 2nd param is token's part-of-speech tag \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blaze']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('blazing', 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fast']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('fastest', 'ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastest\n",
      "fastest\n"
     ]
    }
   ],
   "source": [
    "print(porter_stemmer.stem(\"fastest\"))\n",
    "print(snowball_stemmer.stem(\"fastest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Mountain View GPE\n",
      "California GPE\n",
      "109.65 billion US dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"Google has its headquarters in Mountain View, California having revenue amounted to 109.65 billion US dollars\"\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents: \n",
    "    print(ent.text, ent.label_) \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'I', u'-PRON-', u'PRON', u'PRP', u'nsubj', u'X', True, False)\n",
      "(u'am', u'be', u'VERB', u'VBP', u'aux', u'xx', True, True)\n",
      "(u'learning', u'learn', u'VERB', u'VBG', u'ROOT', u'xxxx', True, False)\n",
      "(u'how', u'how', u'ADV', u'WRB', u'advmod', u'xxx', True, True)\n",
      "(u'to', u'to', u'PART', u'TO', u'aux', u'xx', True, True)\n",
      "(u'build', u'build', u'VERB', u'VB', u'xcomp', u'xxxx', True, False)\n",
      "(u'chatbots', u'chatbot', u'NOUN', u'NNS', u'dobj', u'xxxx', True, False)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am learning how to build chatbots') \n",
    "for token in doc: \n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, \n",
    "        token.shape_, token.is_alpha, token.is_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"2\"><tr><th>TEXT</th><th>LEMMA</th><th>POS</th><th>TAG</th><th>DEP</th><th>SHAPE</th><th>ALPHA</th><th>SHAPE</th></tr><tr><td>I</td><td>-PRON-</td><td>PRON</td><td>PRP</td><td>nsubj</td><td>X</td><td>True</td><td>False</td></tr><tr><td>am</td><td>be</td><td>VERB</td><td>VBP</td><td>aux</td><td>xx</td><td>True</td><td>True</td></tr><tr><td>learning</td><td>learn</td><td>VERB</td><td>VBG</td><td>ROOT</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>how</td><td>how</td><td>ADV</td><td>WRB</td><td>advmod</td><td>xxx</td><td>True</td><td>True</td></tr><tr><td>to</td><td>to</td><td>PART</td><td>TO</td><td>aux</td><td>xx</td><td>True</td><td>True</td></tr><tr><td>build</td><td>build</td><td>VERB</td><td>VB</td><td>xcomp</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>chatbots</td><td>chatbot</td><td>NOUN</td><td>NNS</td><td>dobj</td><td>xxxx</td><td>True</td><td>False</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(u'I', u'-PRON-', u'PRON', u'PRP', u'nsubj', u'X', True, False),\n",
    "(u'am', u'be', u'VERB', u'VBP', u'aux', u'xx', True, True),\n",
    "(u'learning', u'learn', u'VERB', u'VBG', u'ROOT', u'xxxx', True, False),\n",
    "(u'how', u'how', u'ADV', u'WRB', u'advmod', u'xxx', True, True),\n",
    "(u'to', u'to', u'PART', u'TO', u'aux', u'xx', True, True),\n",
    "(u'build', u'build', u'VERB', u'VB', u'xcomp', u'xxxx', True, False),\n",
    "(u'chatbots', u'chatbot', u'NOUN', u'NNS', u'dobj', u'xxxx', True, False)]\n",
    "\n",
    "display_as_table(['TEXT', 'LEMMA', 'POS', 'TAG', 'DEP', 'SHAPE', 'ALPHA', 'SHAPE'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_string = u\"Mark Zuckerberg born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO.\"\n",
    "doc = nlp(my_string) \n",
    "\n",
    "for ent in doc.ents: \n",
    "    print(ent.text, ent.label_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = u\"I usually wake up at 9:00 AM. 90% of my daytime goes in learning new things.\"\n",
    "doc = nlp(my_string) \n",
    "\n",
    "for ent in doc.ents: \n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string1 = u\"Imagine Dragons are the best band.\"\n",
    "my_string2 = u\"Imagine dragons come and take over the city.\"\n",
    "\n",
    "doc1 = nlp(my_string1) \n",
    "doc2 = nlp(my_string2) \n",
    "\n",
    "for ent in doc1.ents: \n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "for ent in doc2.ents: \n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS \n",
    "\n",
    "print(STOP_WORDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[u'is'].is_stop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[u'hello'].is_stop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[u'with'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Book me a flight from Bangalore to Goa') \n",
    "blr, goa = doc[5], doc[7] \n",
    "list(blr.ancestors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[flight, Book]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc[4].ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_ancestor(doc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a, from, to]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc[3].children) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Book a table at the restaurant and the taxi to the hotel')\n",
    "tasks = doc[2], doc[8] #(table, taxi)\n",
    "tasks_target = doc[5], doc[11] #(restaurant, hotel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking of table belongs to restaurant\n",
      "Booking of taxi belongs to hotel\n"
     ]
    }
   ],
   "source": [
    "for task in tasks_target:\n",
    "    for tok in task.ancestors:\n",
    "        if tok in tasks:\n",
    "            print(\"Booking of {} belongs to {}\".format(tok, task))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'dep' visualizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Jul/2018 04:00:04] \"GET / HTTP/1.1\" 200 8843\n",
      "127.0.0.1 - - [23/Jul/2018 04:00:04] \"GET /favicon.ico HTTP/1.1\" 200 8843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy \n",
    "doc = nlp(u'Book a table at the restaurant and the taxi to the hotel')                                               \n",
    "displacy.serve(doc, style='dep') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is referring Berlin to visit\n",
      "User is referring Lubeck to stay\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"What are some places to visit in Berlin and stay in Lubeck\") \n",
    "places = [doc[7], doc[11]] #[Berlin, Lubeck] \n",
    "\n",
    "actions = [doc[5], doc[9]] #[visit, stay] \n",
    "\n",
    "for place in places: \n",
    "    for tok in place.ancestors: \n",
    "        if tok in actions: \n",
    "            print(\"User is referring {} to {}\").format(place, tok) \n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Boston Dynamics, thousands, robot dogs]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"Boston Dynamics is gearing up to produce thousands of robot dogs\") \n",
    "\n",
    "list(doc.noun_chunks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Deep learning', u'learning', u'nsubj', u'cracks')\n",
      "(u'the code', u'code', u'dobj', u'cracks')\n",
      "(u'messenger RNAs', u'RNAs', u'pobj', u'of')\n",
      "(u'protein-coding potential', u'potential', u'conj', u'RNAs')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Deep learning cracks the code of messenger RNAs and protein-coding potential\") \n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"2\"><tr><th>TEXT</th><th>ROOT.TEXT</th><th>ROOT.DEP_</th><th>ROOT.HEAD.TEXT</th></tr><tr><td>Deep learning</td><td>learning</td><td>nsubj</td><td>cracks</td></tr><tr><td>the code</td><td>code</td><td>dobj</td><td>cracks</td></tr><tr><td>messenger RNAs</td><td>RNAs</td><td>pobj</td><td>of</td></tr><tr><td>protein-coding potential</td><td>potential</td><td>conj</td><td>RNAs</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(u'Deep learning', u'learning', u'nsubj', u'cracks'),\n",
    "(u'the code', u'code', u'dobj', u'cracks'),\n",
    "(u'messenger RNAs', u'RNAs', u'pobj', u'of'),\n",
    "(u'protein-coding potential', u'potential', u'conj', u'RNAs')]\n",
    "\n",
    "display_as_table(['TEXT', 'ROOT.TEXT', 'ROOT.DEP_', 'ROOT.HEAD.TEXT'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'How', array([-0.29742685,  0.73939574, -0.04001453,  0.44034013,  2.8967502 ],\n",
      "      dtype=float32))\n",
      "(u'are', array([-0.23435134, -1.6145049 ,  1.0197453 ,  0.9928169 ,  0.28227055],\n",
      "      dtype=float32))\n",
      "(u'you', array([ 0.10252178, -3.564711  ,  2.4822793 ,  4.2824993 ,  3.590245  ],\n",
      "      dtype=float32))\n",
      "(u'doing', array([-0.6240922 , -2.0210216 , -0.91014993,  2.7051923 ,  4.189252  ],\n",
      "      dtype=float32))\n",
      "(u'today', array([ 3.5409122 , -0.62185854,  2.6274266 ,  2.0504875 ,  0.20191991],\n",
      "      dtype=float32))\n",
      "(u'?', array([ 2.8914998 , -0.25079122,  3.3764176 ,  1.6942682 ,  1.9849057 ],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'How are you doing today?') \n",
    "for token in doc: \n",
    "    print(token.text, token.vector[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_doc = nlp(u\"hello\") \n",
    "hi_doc = nlp(u\"hi\") \n",
    "hella_doc = nlp(u\"hella\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7879069442766685\n"
     ]
    }
   ],
   "source": [
    "print hello_doc.similarity(hi_doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4193425861242359\n"
     ]
    }
   ],
   "source": [
    "print hello_doc.similarity(hella_doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.785019122782813"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GoT_str1 = nlp(u\"When will next season of Game of Thrones be releasing?\")\n",
    "GoT_str2 = nlp(u\"Game of Thrones next season release date?\")\n",
    "GoT_str1.similarity(GoT_str2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 100% similar to word car\n",
      "Word car is 71% similar to word truck\n",
      "Word car is 24% similar to word google\n",
      "Word truck is 71% similar to word car\n",
      "Word truck is 100% similar to word truck\n",
      "Word truck is 36% similar to word google\n",
      "Word google is 24% similar to word car\n",
      "Word google is 36% similar to word truck\n",
      "Word google is 100% similar to word google\n"
     ]
    }
   ],
   "source": [
    "example_doc = nlp(u\"car truck google\")\n",
    "\n",
    "for t1 in example_doc:\n",
    "    for t2 in example_doc:\n",
    "        similarity_perc = int(t1.similarity(t2) * 100)\n",
    "        print \"Word {} is {}% similar to word {}\".format(t1.text, similarity_perc,  t2.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit\n",
      "is\n",
      "the\n",
      "impending\n",
      "withdrawal\n",
      "of\n",
      "the\n",
      "U.K.\n",
      "from\n",
      "the\n",
      "European\n",
      "Union\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Brexit is the impending withdrawal of the U.K. from the European Union.')\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Book me a metro from Airport Station to Hong Kong Station.\"\n",
    "sentence2 = \"Book me a cab to Hong Kong Airport from AsiaWorld-Expo.\"\n",
    "\n",
    "import re\n",
    "from_to = re.compile('.* from (.*) to (.*)')\n",
    "to_from = re.compile('.* to (.*) from (.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_to_match = from_to.match(sentence2)\n",
    "to_from_match = to_from.match(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_from pattern matched correctly. Printing values\n",
      "\n",
      "From: AsiaWorld-Expo., To: Hong Kong Airport\n"
     ]
    }
   ],
   "source": [
    "if from_to_match and from_to_match.groups():\n",
    "    _from = from_to_match.groups()[0]\n",
    "    _to = from_to_match.groups()[1]\n",
    "    print(\"from_to pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))\n",
    "    \n",
    "elif to_from_match and to_from_match.groups():\n",
    "    _to = to_from_match.groups()[0]\n",
    "    _from = to_from_match.groups()[1]\n",
    "    print(\"to_from pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
